{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install matplotlib seaborn scikit-learn\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.dropna()  # Drop any rows with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to find outliers boxplot is used\n",
    "sns.boxplot(x=df['Age'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['Income'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['LoanAmount'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['CreditScore'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['MonthsEmployed'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['NumCreditLines'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['InterestRate'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['LoanTerm'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['DTIRatio'])\n",
    "plt.show()\n",
    "sns.boxplot(x=df['Default'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "In this section, we use boxplots to detect outliers in various numeric columns of the dataset. Boxplots are a useful visualization tool for identifying outliers, as they display the distribution of data and highlight any values that fall significantly outside the expected range. The columns analyzed for outliers include:\n",
    "\n",
    "- Age\n",
    "- Income\n",
    "- LoanAmount\n",
    "- CreditScore\n",
    "- MonthsEmployed\n",
    "- NumCreditLines\n",
    "- InterestRate\n",
    "- LoanTerm\n",
    "- DTIRatio\n",
    "- Default\n",
    "\n",
    "By examining these boxplots, we can identify and potentially address any outliers that may affect the performance of our machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preprocessing and Feature Engineering\n",
    "\n",
    "In this section, we perform data preprocessing and feature engineering to prepare the dataset for machine learning models. The steps include:\n",
    "\n",
    "1. **Selecting Numeric Columns**: We filter the dataframe to include only columns with numeric data types (`float64` and `int64`).\n",
    "\n",
    "2. **Dropping Unnecessary Columns**: We drop the `LoanID` column as it is an identifier and not useful for modeling.\n",
    "\n",
    "3. **Separating Features and Target**: We separate the features (predictor variables) and the target variable (`Default`).\n",
    "\n",
    "4. **Standardizing the Features**: We scale the features to have a mean of 0 and a standard deviation of 1 using `StandardScaler`.\n",
    "\n",
    "5. **Combining Scaled Features and Target**: We combine the scaled features with the target variable to form a new dataframe.\n",
    "\n",
    "6. **Ensuring Binary Target**: We check that the `Default` column contains only binary values (0 or 1).\n",
    "\n",
    "7. **Summary Statistics**: We display the summary statistics of the scaled dataframe to understand the distributions and statistics of all columns.\n",
    "\n",
    "These preprocessing steps are crucial for ensuring that the data is clean, consistent, and suitable for training machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns\n",
    "# Here we filter the dataframe `df` to include only columns with numeric data types (i.e., float64 and int64).\n",
    "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Correlation matrix and heatmap\n",
    "# Calculate the correlation matrix for the scaled dataframe. This matrix shows the linear relationships between all pairs of features.\n",
    "corr = df_scaled.corr()  # Calculate the correlation matrix for the scaled features and target variable.\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "# Create a heatmap visualization to easily understand the relationships between variables.\n",
    "# The `annot=True` option shows the correlation coefficient values in the heatmap, while `fmt=\".6f\"` formats them to 6 decimal places.\n",
    "# The `cmap='coolwarm'` sets the color scheme, and `linewidths=0.5` adds some separation between the cells for clarity.\n",
    "plt.figure(figsize=(12, 10))  # Set the figure size for better readability of the heatmap.\n",
    "sns.heatmap(corr, annot=True, fmt=\".6f\", cmap='coolwarm', linewidths=0.5)  # Generate the heatmap with the correlation matrix.\n",
    "plt.show()  # Display the heatmap.\n",
    "\n",
    "# Print the correlation matrix\n",
    "# This prints the raw correlation matrix values, so you can also inspect the relationships between features in the console.\n",
    "print(corr)\n",
    "\n",
    "# Drop the columns that have bad correlation if they exist\n",
    "# Sometimes, highly correlated features (multicollinearity) can harm the performance of certain models (e.g., linear regression).\n",
    "# In this step, you could decide to drop such columns. \n",
    "# The example here shows columns 'MonthsEmployed' and 'NumCreditLines' as potential candidates for removal.\n",
    "# However, we have commented it out for now, and the actual columns to drop can be decided based on the correlation matrix.\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "columns_to_drop = ['MonthsEmployed']#, 'NumCreditLines']  # Example columns that might have high correlation with others.\n",
    "df_scaled = df_scaled.drop(columns=[col for col in columns_to_drop if col in df_scaled.columns])  # Drop the selected columns.\n",
    "#did not drop any columns cause they gave worse results\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Display the summary statistics of the scaled DataFrame\n",
    "# This will give us an overview of the data distributions after scaling (mean, standard deviation, min, max, etc.).\n",
    "# It helps us check for any extreme outliers or issues with the data after scaling.\n",
    "print(df_scaled.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Correlation Analysis\n",
    "\n",
    "In this section, we perform correlation analysis to understand the relationships between different numeric features in the dataset. The steps include:\n",
    "\n",
    "1. **Selecting Numeric Columns**: We filter the dataframe to include only columns with numeric data types (`float64` and `int64`).\n",
    "\n",
    "2. **Calculating Correlation Matrix**: We compute the correlation matrix for the scaled dataframe to identify linear relationships between pairs of features.\n",
    "\n",
    "3. **Plotting Correlation Heatmap**: We visualize the correlation matrix using a heatmap, which helps in easily identifying strong positive or negative correlations between features.\n",
    "\n",
    "4. **Dropping Highly Correlated Features**: If necessary, we drop features that are highly correlated with others to avoid multicollinearity, which can negatively impact the performance of certain machine learning models.\n",
    "\n",
    "5. **Summary Statistics**: We display the summary statistics of the scaled dataframe to understand the distributions and statistics of all columns after scaling.\n",
    "\n",
    "By analyzing the correlations, we can make informed decisions about feature selection and engineering, which are crucial for building robust machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the dataset\n",
    "# Before proceeding with machine learning, it's important to check for missing values (NaNs) in the dataset.\n",
    "# If there are NaNs, we need to handle them through imputation, removal, or other techniques.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop('Default', axis=1), df_scaled['Default'].astype(int), test_size=0.01, random_state=87)\n",
    "# Split the scaled dataset into training and test sets. We are dropping 'Default' from X (features) and using it as y (target).\n",
    "# 1% of the data is allocated to the test set using `test_size=0.01`, and the random state ensures reproducibility of the split.\n",
    "\n",
    "# Check for NaN values in X_train (features)\n",
    "# Here, we check if there are any NaN values in the training features (X_train).\n",
    "print(\"NaN values in X_train:\", X_train.isnull().sum().sum())  # `.isnull()` checks for NaNs, `.sum()` counts them, and `.sum().sum()` gives the total number of NaNs in the dataset.\n",
    "\n",
    "# Check for NaN values in y_train (target)\n",
    "# We also check for NaN values in the target variable (y_train). Missing target values can cause issues during model training.\n",
    "print(\"NaN values in y_train:\", y_train.isnull().sum())  # `.isnull()` checks for NaNs, `.sum()` counts them in y_train.\n",
    "\n",
    "# Check for non-numeric data types in X_train (features)\n",
    "# Since machine learning models typically require numeric inputs, it's important to verify that there are no non-numeric columns in the feature set.\n",
    "# This step is crucial, especially after preprocessing, to ensure that only numeric columns are present in the training set.\n",
    "print(\"Data types in X_train:\", X_train.dtypes)  # `.dtypes` will show the data types of each column in X_train.\n",
    "\n",
    "# Check the data type in y_train (target)\n",
    "# Ensure the target variable y_train is in the correct data type (usually int or float for classification).\n",
    "# For classification, it's important that the target is either integer (for class labels) or float.\n",
    "print(\"Data type in y_train:\", y_train.dtype)  # `.dtype` will display the data type of the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Train-Test Split and Data Validation Grid Search\n",
    "\n",
    "In this section, we perform the following steps to prepare our data for machine learning:\n",
    "\n",
    "1. **Train-Test Split**: We split the scaled dataset into training and test sets. We use 99% of the data for training and 1% for testing to ensure we have enough data for model training while keeping a small portion for validation.\n",
    "\n",
    "2. **Check for NaN Values**: We check for any missing values (NaNs) in both the training features (`X_train`) and the target variable (`y_train`). Handling NaNs is crucial as they can cause issues during model training.\n",
    "\n",
    "3. **Check Data Types**: We verify that all columns in the training features (`X_train`) are numeric, as most machine learning models require numeric inputs. We also ensure that the target variable (`y_train`) is of the correct data type (integer) for classification.\n",
    "\n",
    "4. **Grid Search for Hyperparameter Tuning**: We use GridSearchCV to perform hyperparameter tuning for our Random Forest model. Grid search tests multiple combinations of hyperparameters to find the best configuration for our model. We define a parameter grid and use 3-fold cross-validation to evaluate each combination.\n",
    "\n",
    "5. **Evaluate Best Model**: After finding the best hyperparameters, we evaluate the best model on the test set to check its performance. We calculate the accuracy of the model's predictions on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch\n",
    "\n",
    "# Prepare the data, ensuring y is integer type\n",
    "X = df_scaled.drop('Default', axis=1)           # Define features (X) from the DataFrame\n",
    "y = df_scaled['Default'].astype(int)             # Convert the target variable to integer type\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Here we use 80% of the data for training and 20% for testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],              # Number of trees in the forest\n",
    "    'max_depth': [2, 5, 10],                     # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],             # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]                # Minimum samples required to be a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "# 'class_weight=\"balanced\"' helps handle class imbalance by adjusting weights\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Set up GridSearchCV\n",
    "# Grid search will test all parameter combinations specified in param_grid\n",
    "# 'cv=3' means 3-fold cross-validation, 'n_jobs=-1' uses all CPU cores, and 'verbose=2' for progress output\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with GridSearchCV to find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and model found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Estimator:\", grid_search.best_estimator_)\n",
    "print(\"Best Cross-validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Print detailed results for each parameter combination\n",
    "print(\"Grid Search Results:\", grid_search.cv_results_)               # Complete details of all configurations\n",
    "print(\"Mean Test Scores:\", grid_search.cv_results_['mean_test_score']) # Mean cross-validation scores for each configuration\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "# grid_search.best_estimator_ contains the optimized Random Forest model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)                # Predict on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)       # Calculate accuracy of predictions\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search submission\n",
    "# Load and preprocess the test dataset\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "df_test_numeric = df_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Match test data columns to training feature columns\n",
    "# Drop any columns in df_test_numeric that are not in features_scaled.columns\n",
    "df_test_numeric = df_test_numeric[features.columns]  # Ensure test columns align with training features\n",
    "\n",
    "# Standardize the test data using the fitted scaler\n",
    "df_test_scaled = scaler.transform(df_test_numeric)\n",
    "df_test_scaled = pd.DataFrame(df_test_scaled, columns=features.columns)\n",
    "\n",
    "# Drop the columns that were dropped during training\n",
    "columns_to_drop = ['MonthsEmployed', 'NumCreditLines']\n",
    "df_test_scaled = df_test_scaled.drop(columns=[col for col in columns_to_drop if col in df_test_scaled.columns])\n",
    "\n",
    "# Check for NaN values in the test dataset\n",
    "print(\"NaN values in df_test_scaled:\", df_test_scaled.isnull().sum().sum())\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = best_rf.predict(df_test_scaled)\n",
    "\n",
    "# Add the predictions to the original test DataFrame\n",
    "df_test['Default'] = y_pred\n",
    "\n",
    "# Select only the LoanID and Default columns for submission\n",
    "df_submission = df_test[['LoanID', 'Default']]\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "df_submission.to_csv('sub.csv', index=False)\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(df_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost with Grid Search\n",
    "\n",
    "In this section, we utilize XGBoost, a powerful gradient boosting algorithm, along with Grid Search for hyperparameter tuning. The steps include:\n",
    "\n",
    "1. **Data Preparation**: We prepare the data by separating the features and target variable, and splitting the dataset into training and test sets.\n",
    "\n",
    "2. **Defining Parameter Grid**: We define a parameter grid for XGBoost, specifying different values for hyperparameters such as the number of trees (`n_estimators`), learning rate (`learning_rate`), maximum depth of trees (`max_depth`), subsampling rate (`subsample`), and column sampling rate (`colsample_bytree`).\n",
    "\n",
    "3. **Initializing XGBoost Classifier**: We initialize the XGBoost classifier with GPU support to speed up the training process.\n",
    "\n",
    "4. **Grid Search with Cross-Validation**: We set up `GridSearchCV` to perform an exhaustive search over the specified parameter grid, using 3-fold cross-validation to evaluate each combination of hyperparameters.\n",
    "\n",
    "5. **Training the Model**: We fit the model using `GridSearchCV` to find the best hyperparameters and train the XGBoost model.\n",
    "\n",
    "6. **Evaluating the Best Model**: After finding the best hyperparameters, we evaluate the best model on the test set to check its performance. We calculate the accuracy of the model's predictions on the test data.\n",
    "\n",
    "By using Grid Search with XGBoost, we aim to optimize the model's performance by finding the best combination of hyperparameters, ensuring robust and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xg+grid\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare the data\n",
    "X = df_scaled.drop('Default', axis=1)          # Select features for training\n",
    "y = df_scaled['Default'].astype(int)            # Convert the target variable to integer type\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a balanced parameter grid for XGBoost\n",
    "# param_grid_xgb = {\n",
    "#     'n_estimators': [50, 100, 150],             # Three options for number of trees\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],          # Three values for learning rate\n",
    "#     'max_depth': [3, 5, 7],                     # Three levels for max depth\n",
    "#     'subsample': [0.8, 1.0],                    # Two options for subsampling rate\n",
    "#     'colsample_bytree': [0.8, 1.0]              # Two options for column sampling\n",
    "# }\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 150],               # Adding an extra value for number of trees\n",
    "    'learning_rate': [0.05, 0.1, 0.2],            # Adding a lower learning rate for finer control\n",
    "    'max_depth': [3, 5, 7],                       # Expanding max depth options for model complexity\n",
    "    'subsample': [0.7, 0.8, 1.0],                 # Adding more values for subsampling rate\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]           # Adding more values for column sampling\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize the XGBoost Classifier with GPU support\n",
    "xgb_clf = xgb.XGBClassifier(tree_method='gpu_hist', random_state=19)\n",
    "\n",
    "# Set up GridSearchCV with XGBoost\n",
    "grid_search_gb = GridSearchCV(estimator=xgb_clf, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model with GridSearchCV\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score found by GridSearchCV\n",
    "print(\"Best Parameters for XGBoost:\", grid_search_gb.best_params_)\n",
    "print(\"Best Estimator for XGBoost:\", grid_search_gb.best_estimator_)\n",
    "print(\"Best Cross-validation Score for XGBoost:\", grid_search_gb.best_score_)\n",
    "\n",
    "# Define best_gb as the best estimator from grid search\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_xgb = best_gb.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"Test Accuracy with XGBoost:\", accuracy_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xg+grid submission no columsn dropped\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "df_test_numeric = df_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Match test data columns to training feature columns\n",
    "# Drop any columns in df_test_numeric that are not in features_scaled.columns\n",
    "df_test_numeric = df_test_numeric[features.columns]  # Ensure test columns align with training\n",
    "\n",
    "# Standardize the test data using the fitted scaler\n",
    "df_test_scaled = scaler.transform(df_test_numeric)\n",
    "df_test_scaled = pd.DataFrame(df_test_scaled, columns=features.columns)\n",
    "#drop the columns that have bad correlation if they exist\n",
    "columns_to_drop = ['MonthsEmployed', 'NumCreditLines']\n",
    "df_test_scaled = df_test_scaled.drop(columns=[col for col in columns_to_drop if col in df_test_scaled.columns])\n",
    "# Check for NaN values in the test dataset\n",
    "print(\"NaN values in df_test_scaled:\", df_test_scaled.isnull().sum().sum())\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred_gb = best_gb.predict(df_test_scaled)\n",
    "\n",
    "# Add the predictions to the original test DataFrame\n",
    "df_test['Default'] = y_pred_gb\n",
    "\n",
    "# Select only the LoanID and Default columns for submission\n",
    "df_submission_gb = df_test[['LoanID', 'Default']]\n",
    "# Save the submission DataFrame to a CSV file\n",
    "df_submission_gb.to_csv('sub_gb.csv', index=False)\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(df_submission_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model Training and Evaluation\n",
    "\n",
    "In this section, we focus on training and evaluating an XGBoost model. The steps include:\n",
    "\n",
    "1. **Data Preparation**: We prepare the data by separating the features and target variable, and splitting the dataset into training and test sets.\n",
    "\n",
    "2. **Defining XGBoost Parameters**: We define a set of hyperparameters for the XGBoost model, including the number of trees (`n_estimators`), learning rate (`learning_rate`), maximum depth of trees (`max_depth`), subsampling rate (`subsample`), column sampling rate (`colsample_bytree`), and regularization parameters (`gamma`, `min_child_weight`, `reg_alpha`, `reg_lambda`).\n",
    "\n",
    "3. **Training the XGBoost Model**: We initialize the XGBoost classifier with the defined parameters and train it on the training data.\n",
    "\n",
    "4. **Evaluating the Model**: We evaluate the trained XGBoost model on the test set by predicting the target variable and calculating the accuracy of the predictions.\n",
    "\n",
    "By using XGBoost, we aim to leverage its powerful gradient boosting algorithm to achieve robust and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 16\n",
    "#xgbost with no dropped columsn s\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop('Default', axis=1), df_scaled['Default'], test_size=0.01, random_state=87)\n",
    "\n",
    "# Define a more complex and fine-tuned XGBoost model\n",
    "xgb_params = {\n",
    "    'n_estimators': 1000,                        # Use more trees to improve model complexity\n",
    "    'learning_rate': 0.01,                       # Lower learning rate for better optimization\n",
    "    'max_depth': 12,                             # Set deeper trees for more complex patterns\n",
    "    'subsample': 0.85,                           # Slightly lower subsample to avoid overfitting\n",
    "    'colsample_bytree': 0.85,                    # Use 85% of the features per tree\n",
    "    'gamma': 5,                                  # Stronger regularization to prevent overfitting\n",
    "    'min_child_weight': 7,                       # Prevent overfitting by increasing min_child_weight\n",
    "    'reg_alpha': 0.5,                            # L1 regularization\n",
    "    'reg_lambda': 1,                             # L2 regularization\n",
    "    'tree_method': 'gpu_hist',                   # Use GPU for faster training if available\n",
    "    'random_state': 87                           # Ensure reproducibility\n",
    "}\n",
    "\n",
    "#drop the columns that have bad correlation if they exist\n",
    "# columns_to_drop = ['MonthsEmployed', 'NumCreditLines']\n",
    "# df_scaled = df_scaled.drop(columns=[col for col in columns_to_drop if col in df_scaled.columns])\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Save the model to best_gb\n",
    "best_gb = xgb_clf\n",
    "\n",
    "# Predict the target variable on the test data\n",
    "y_pred_xgb = best_gb.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"Test Accuracy with XGBoost:\", accuracy_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 17\n",
    "#xg submission\n",
    "# Now use the trained model to predict on the test data (df_test)\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Ensure all columns are numeric and align with the features in the training set\n",
    "df_test_numeric = df_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Drop 'LoanID' from the test set as well (it is not needed for prediction)\n",
    "#df_test_numeric = df_test_numeric.drop('LoanID', axis=1)\n",
    "\n",
    "# Standardize the test data\n",
    "df_test_scaled = scaler.transform(df_test_numeric)\n",
    "df_test_scaled = pd.DataFrame(df_test_scaled, columns=features.columns)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# Drop the columns that were dropped during training\n",
    "# columns_to_drop = ['MonthsEmployed', 'NumCreditLines']\n",
    "# df_test_scaled = df_test_scaled.drop(columns=[col for col in columns_to_drop if col in df_test_scaled.columns])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Check for NaN values in the test dataset\n",
    "print(\"NaN values in df_test_scaled:\", df_test_scaled.isnull().sum().sum())\n",
    "\n",
    "# Predict the target variable for the test set\n",
    "y_pred_gb = best_gb.predict(df_test_scaled)\n",
    "\n",
    "# Add the predictions to the original test DataFrame\n",
    "df_test['Default'] = y_pred_gb\n",
    "\n",
    "# Select only the LoanID and Default columns for submission\n",
    "df_submission_gb = df_test[['LoanID', 'Default']]\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "df_submission_gb.to_csv('sub_gb2.csv', index=False)\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(df_submission_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensemble Model\n",
    "\n",
    "In this section, we implement a stacking ensemble model to improve the prediction accuracy by combining multiple base models. The steps include:\n",
    "\n",
    "1. **Data Preparation**: We prepare the data by separating the features and target variable, and splitting the dataset into training and test sets.\n",
    "\n",
    "2. **Defining Base Models**: We define two base models for the stacking ensemble:\n",
    "    - Random Forest Classifier\n",
    "    - Gradient Boosting Classifier\n",
    "\n",
    "3. **Stacking Classifier**: We create a stacking classifier that combines the predictions from the base models using a meta-model (Logistic Regression).\n",
    "\n",
    "4. **Training the Stacking Model**: We fit the stacking model on the training data.\n",
    "\n",
    "5. **Evaluating the Stacking Model**: We evaluate the stacking model on the test set by predicting the target variable and calculating the accuracy of the predictions.\n",
    "\n",
    "By using a stacking ensemble, we aim to leverage the strengths of multiple models to achieve better performance and more robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking\n",
    "# Prepare the data\n",
    "# Separate features and target variable from the scaled DataFrame\n",
    "X = df_scaled.drop('Default', axis=1)\n",
    "y = df_scaled['Default'].astype(int)  # Convert target variable to integer type for classification\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Using 80% of data for training and 20% for testing, with a fixed random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models for stacking\n",
    "base_estimators = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),  # Random Forest model\n",
    "    ('gradient_boosting', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42))  # Gradient Boosting model\n",
    "]\n",
    "\n",
    "# Define the stacking classifier\n",
    "# This ensemble model combines predictions from base models using a meta-model (Logistic Regression)\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,               # Base models in the stacking ensemble\n",
    "    final_estimator=LogisticRegression(),     # Meta-model to combine predictions from base models\n",
    "    cv=3,                                     # 3-fold cross-validation for meta-model training\n",
    "    n_jobs=-1                                 # Use all available cores for parallel processing\n",
    ")\n",
    "\n",
    "# Fit the stacking model on the training data\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target on the test set using the trained stacking model\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy to evaluate the stacking model's performance on the test set\n",
    "stack_accuracy = accuracy_score(y_test, y_pred_stack)\n",
    "print(\"Test Accuracy with Stacking Ensemble:\", stack_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset and drop any rows with missing values\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "# Select only numeric columns from the test dataset\n",
    "df_test_numeric = df_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# Match test data columns to training feature columns by selecting only columns present in `features`\n",
    "# This ensures the test data has the same columns as the training data used in modeling\n",
    "df_test_numeric = df_test_numeric[features.columns]  # Ensure test columns align with training\n",
    "\n",
    "# Standardize the test data using the previously fitted scaler\n",
    "# The scaler object was trained on the training data and now transforms the test data to match that scaling\n",
    "df_test_scaled = scaler.transform(df_test_numeric)\n",
    "df_test_scaled = pd.DataFrame(df_test_scaled, columns=features.columns)\n",
    "\n",
    "# Drop any columns that had low correlation and were dropped from the training dataset, if they are present\n",
    "# This keeps the test set consistent with the model’s feature expectations\n",
    "columns_to_drop = ['MonthsEmployed', 'NumCreditLines']\n",
    "df_test_scaled = df_test_scaled.drop(columns=[col for col in columns_to_drop if col in df_test_scaled.columns])\n",
    "\n",
    "# Check if there are any NaN values in the test dataset after processing\n",
    "print(\"NaN values in df_test_scaled:\", df_test_scaled.isnull().sum().sum())\n",
    "\n",
    "# Use the stacking model to predict the 'Default' status on the test dataset\n",
    "# This model combines predictions from Random Forest and Gradient Boosting\n",
    "y_pred_stack = stacking_model.predict(df_test_scaled)\n",
    "\n",
    "# Add the predictions as a new column in the original test DataFrame\n",
    "df_test['Default'] = y_pred_stack\n",
    "\n",
    "# Select only the LoanID and Default columns for the final submission\n",
    "df_submission_stack = df_test[['LoanID', 'Default']]\n",
    "\n",
    "# Save the prepared submission DataFrame to a CSV file\n",
    "# The CSV can be submitted with predictions for each loan in the test dataset\n",
    "df_submission_stack.to_csv('sub_stack.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
